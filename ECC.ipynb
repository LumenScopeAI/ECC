{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECC\n",
    "## Train 不同精度模型\n",
    "\n",
    "按照FP32、BF32、FP16、Int8、BF16进行Train \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Int8Train 在单独的cell中，使用了huggingface用于给LLM做Int8量化的包（对于resnet一样兼容）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 设置Train 参数\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 2e-5\n",
    "momentum = 0.9\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 调整图像大小为224x224\n",
    "    transforms.Grayscale(num_output_channels=3),  # 将单通道图像扩展为三通道\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载MNIST数据集\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义Train 函数\n",
    "def train(model, device, precision,ifint8=False):\n",
    "    # 设置模型为Train 模式\n",
    "    model.train()\n",
    "    \n",
    "    # 创建优化器\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    # 创建Loss函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 创建Tensorboard日志写入器\n",
    "    log_dir = f\"/root/autodl-tmp/log/{precision}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    # Train 循环\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if precision in ['bf16', 'bf32']:\n",
    "                data = data.bfloat16()\n",
    "            elif precision == 'fp16':\n",
    "                data = data.half()\n",
    "            \n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 记录Train Loss\n",
    "            writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "        \n",
    "        # 在Test 集上评估模型\n",
    "        test(model, device, precision, epoch, writer)\n",
    "        # 保存模型参数\n",
    "        checkpoint_dir = f\"/root/autodl-tmp/checkpoint/{precision}\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), f\"{checkpoint_dir}/model_epoch_{epoch}.pth\")\n",
    "\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# 定义Test 函数\n",
    "def test(model, device, precision, epoch, writer):\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 初始化Test Loss和Accuracy\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # 创建Loss函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if precision in ['bf16', 'bf32']:\n",
    "                data = data.bfloat16()\n",
    "            elif precision == 'fp16':\n",
    "                data = data.half()\n",
    "            \n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    # 计算平均Test Loss和Accuracy\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    # 记录Test Loss和Accuracy\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/test', accuracy, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Precision {precision}, Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Train 和Test 模型\n",
    "precisions = ['fp32', 'bf32', 'fp16','bf16']\n",
    "\n",
    "for precision in precisions:\n",
    "    # 创建模型\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=10)\n",
    "    \n",
    "    # 将模型移动到指定设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # 设置模型精度\n",
    "    if precision == 'fp32':\n",
    "        model.float()\n",
    "    elif precision == 'bf32':\n",
    "        model.bfloat16()\n",
    "    elif precision == 'fp16':\n",
    "        model.half()\n",
    "    elif precision == 'bf16':\n",
    "        model.bfloat16()\n",
    "    # Train 模型(除了int8以外的精度)\n",
    "    train(model, device, precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对网络参数进行随机扰动\n",
    "\n",
    "随机扰动：遍历模型，对每个参数按照比特位进行扰动。\n",
    "\n",
    "1. FP32使用32位来表示每个参数，其中1位用于符号位，8位用于指数位，23位用于尾数位。\n",
    "\n",
    "2. BF32是英特尔提出的一种优化的浮点格式，也使用32位来表示每个参数，与FP32相比，BF32减少了指数位的数量，增加了尾数位的数量，BF32使用1位表示符号位，8位表示指数位，23位表示尾数位。\n",
    "\n",
    "3. FP16使用16位来表示每个参数，其中1位用于符号位，5位用于指数位，10位用于尾数位。\n",
    "\n",
    "4. INT8使用8位来表示每个参数，INT8表示有符号整数，范围从-128到127。\n",
    "\n",
    "5. BF16是Google提出的一种优化的浮点格式，使用16位来表示每个参数，与FP16相比，BF16减少了指数位的数量，增加了尾数位的数量，BF16使用1位表示符号位，8位表示指数位，7位表示尾数位。\n",
    "\n",
    "-   FP32和BF32使用32位表示每个参数。\n",
    "-   FP16和BF16使用16位表示每个参数。\n",
    "-   INT8使用8位表示每个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32\n",
    "\n",
    "FP32使用32位来表示每个参数，其中1位用于符号位，8位用于指数位，23位用于尾数位。\n",
    "\n",
    "1. 引入四个变量，ERR_Rate表示引入错误的概率，从[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]取值，剩余三个变量为布尔型：ERR_01、ERR_02、ERR_03，用于控制符号位是否发生错误（bit翻转）、指数位是否发生错误（bit翻转）、尾数位是否发生错误（bit翻转）。\n",
    "\n",
    "2. 对模型进行随机错误注入，将注入错误的模型保存在/media/tangshi/AI0011/笔记/信工所/ECC校验_正式/ERR_checkpoint对应的子文件夹下，如果子文件夹不存在则新建。\n",
    "\n",
    "3. ERR_01、ERR_02、ERR_03分别单独设为True进行实验，也就是说，最终注入错误后你需要保存3*9=27个不同错误率的、不同错误发生位置的模型。\n",
    "\n",
    "目前存在待优化的问题：\n",
    "\n",
    "1. 即便多进程进行错误注入还是很慢，跑了好几天才把fp32的注入完，因此后续几种精度暂时只跑了注入一个错误的Test 代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于任何非INT8的数，我们采用以下新的基于奇偶的ECC鉴别纠错：\n",
    "众所周知，所有的数据在展开到bit级别都只有0和1，我们假设这是一串二进制编码，将它转化成整数之后就可以判别奇偶性。\n",
    "因此，我基于此思想设计了一种新的ECC显存校验机制，我们通过对参数展开后的基础性进行分析，如果奇偶性正确，虽然这个参数还有可能出错，但我们判断这个参数是正确的。如果奇偶性错误，那么这个参数必然出错，我们将这个参数设为零。\n",
    "同样的，Int8数据类型的参数就更加适用了！\n",
    "请你记住这个想法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 设置Train 参数\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "learning_rate = 2e-5\n",
    "momentum = 0.9\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 调整图像大小为224x224\n",
    "    transforms.Grayscale(num_output_channels=3),  # 将单通道图像扩展为三通道\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载MNIST数据集\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 定义Train 函数\n",
    "def train(model, device, precision,ifint8=False):\n",
    "    # 设置模型为Train 模式\n",
    "    model.train()\n",
    "    \n",
    "    # 创建优化器\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    # 创建Loss函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 创建Tensorboard日志写入器\n",
    "    log_dir = f\"/root/autodl-tmp/log/{precision}\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    # Train 循环\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if precision in ['bf16', 'bf32']:\n",
    "                data = data.bfloat16()\n",
    "            elif precision == 'fp16':\n",
    "                data = data.half()\n",
    "            \n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 记录Train Loss\n",
    "            writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "        \n",
    "        # 在Test 集上评估模型\n",
    "        test(model, device, precision, epoch, writer)\n",
    "        # 保存模型参数\n",
    "        checkpoint_dir = f\"/root/autodl-tmp/checkpoint/{precision}\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), f\"{checkpoint_dir}/model_epoch_{epoch}.pth\")\n",
    "\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# 定义Test 函数\n",
    "def test(model, device, precision, epoch, writer):\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 初始化Test Loss和ACC\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # 创建Loss函数\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            if precision in ['bf16', 'bf32']:\n",
    "                data = data.bfloat16()\n",
    "            elif precision == 'fp16':\n",
    "                data = data.half()\n",
    "            \n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    # 计算平均Test Loss和Accuracy\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    # 记录Test Loss和Accuracy\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/test', accuracy, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Precision {precision}, Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Train 和Test 模型\n",
    "precisions = ['fp32', 'bf32', 'fp16','bf16']\n",
    "\n",
    "for precision in precisions:\n",
    "    # 创建模型\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=10)\n",
    "    \n",
    "    # 将模型移动到指定设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # 设置模型精度\n",
    "    if precision == 'fp32':\n",
    "        model.float()\n",
    "    elif precision == 'bf32':\n",
    "        model.bfloat16()\n",
    "    elif precision == 'fp16':\n",
    "        model.half()\n",
    "    elif precision == 'bf16':\n",
    "        model.bfloat16()\n",
    "    # Train 模型(除了int8以外的精度)\n",
    "    train(model, device, precision)\n",
    "\n",
    "```\n",
    "以上为Train 代码\n",
    "```\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "precision = 'fp32'\n",
    "epochs = 1\n",
    "checkpoint_dir = f\"/root/autodl-tmp/checkpoint/{precision}\"\n",
    "model_path = f\"{checkpoint_dir}/model_epoch_{epochs-1}.pth\"\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=10)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device).float()\n",
    "\n",
    "ERR_Rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "def inject_error(tensor, err_rate, err_01, err_02, err_03):\n",
    "    tensor_copy = tensor.clone().cpu().numpy()  # Move tensor to CPU for numpy operations\n",
    "    total_bits = tensor_copy.nbytes * 8\n",
    "    num_errors = int(total_bits * err_rate)\n",
    "\n",
    "    error_indices = []\n",
    "    tensor_bytes = tensor_copy.tobytes()\n",
    "    tensor_int8 = np.frombuffer(tensor_bytes, dtype=np.int8)\n",
    "    tensor_int8_writable = np.copy(tensor_int8)  # 创建一个可写的副本\n",
    "\n",
    "    for _ in range(num_errors):\n",
    "        bit_pos = random.randint(0, total_bits - 1)\n",
    "        byte_pos, bit_offset = divmod(bit_pos, 8)\n",
    "\n",
    "        if err_01 and bit_offset == 31:  # 符号位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "        elif err_02 and 23 <= bit_offset <= 30:  # 指数位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "        elif err_03 and bit_offset < 23:  # 尾数位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "\n",
    "    tensor_copy = np.frombuffer(tensor_int8_writable.tobytes(), dtype=tensor_copy.dtype).reshape(tensor_copy.shape)\n",
    "    tensor_copy_writable = np.copy(tensor_copy)  # 创建一个可写的副本\n",
    "    return torch.from_numpy(tensor_copy_writable).to(device), error_indices  # Move tensor back to the original device\n",
    "\n",
    "def save_error_injected_model(model, err_rate, err_01, err_02, err_03):\n",
    "    err_checkpoint_dir = f\"/root/autodl-tmp/ERR_checkpoint/FP32/ERR_Rate_{err_rate}\"\n",
    "    os.makedirs(err_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    model_state_dict = model.state_dict()\n",
    "    error_dict = {}\n",
    "    for key, tensor in model_state_dict.items():\n",
    "        model_state_dict[key], error_indices = inject_error(tensor, err_rate, err_01, err_02, err_03)\n",
    "        error_dict[key] = error_indices\n",
    "\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    torch.save(model.state_dict(), f\"{err_checkpoint_dir}/model_ERR_01_{err_01}_ERR_02_{err_02}_ERR_03_{err_03}.pth\")\n",
    "\n",
    "    with open(f\"{err_checkpoint_dir}/error_log_ERR_01_{err_01}_ERR_02_{err_02}_ERR_03_{err_03}.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Parameter', 'Error Indices'])\n",
    "        for key, indices in error_dict.items():\n",
    "            writer.writerow([key, indices])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for err_rate in ERR_Rates:\n",
    "        print(f\"Processing error rate: {err_rate}\")\n",
    "        save_error_injected_model(model, err_rate, True, False, False)  # ERR_01 = True\n",
    "        save_error_injected_model(model, err_rate, False, True, False)  # ERR_02 = True\n",
    "        save_error_injected_model(model, err_rate, False, False, True)  # ERR_03 = True\n",
    "    \n",
    "    print(\"All experiments completed.\")\n",
    "```\n",
    "FP32使用32位来表示每个参数，其中1位用于符号位，8位用于指数位，23位用于尾数位。\n",
    "\n",
    "1. 引入四个变量，ERR_Rate表示引入错误的概率，从[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]取值，剩余三个变量为布尔型：ERR_01、ERR_02、ERR_03，用于控制符号位是否发生错误（bit翻转）、指数位是否发生错误（bit翻转）、尾数位是否发生错误（bit翻转）。\n",
    "\n",
    "2. 对模型进行随机错误注入，将注入错误的模型保存在/media/tangshi/AI0011/笔记/信工所/ECC校验_正式/ERR_checkpoint对应的子文件夹下，如果子文件夹不存在则新建。\n",
    "\n",
    "3. ERR_01、ERR_02、ERR_03分别单独设为True进行实验，也就是说，最终注入错误后你需要保存3*9=27个不同错误率的、不同错误发生位置的模型。\n",
    "\n",
    "目前存在待优化的问题：\n",
    "\n",
    "1. 即便多进程进行错误注入还是很慢，跑了好几天才把fp32的注入完，因此后续几种精度暂时只跑了注入一个错误的Test 代码。\n",
    "\n",
    "以上就是对于FP32数据的错误注入代码。\n",
    "\n",
    "你需要做的是：\n",
    "使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "precision = 'fp32'\n",
    "epochs = 1\n",
    "checkpoint_dir = f\"/root/autodl-tmp/checkpoint/{precision}\"\n",
    "model_path = f\"{checkpoint_dir}/model_epoch_{epochs-1}.pth\"\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=10)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device).float()\n",
    "\n",
    "ERR_Rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "def inject_error(tensor, err_rate, err_01, err_02, err_03):\n",
    "    tensor_copy = tensor.clone().cpu().numpy()  # Move tensor to CPU for numpy operations\n",
    "    total_bits = tensor_copy.nbytes * 8\n",
    "    num_errors = int(total_bits * err_rate)\n",
    "\n",
    "    error_indices = []\n",
    "    tensor_bytes = tensor_copy.tobytes()\n",
    "    tensor_int8 = np.frombuffer(tensor_bytes, dtype=np.int8)\n",
    "    tensor_int8_writable = np.copy(tensor_int8)  # 创建一个可写的副本\n",
    "\n",
    "    for _ in range(num_errors):\n",
    "        bit_pos = random.randint(0, total_bits - 1)\n",
    "        byte_pos, bit_offset = divmod(bit_pos, 8)\n",
    "\n",
    "        if err_01 and bit_offset == 31:  # 符号位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "        elif err_02 and 23 <= bit_offset <= 30:  # 指数位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "        elif err_03 and bit_offset < 23:  # 尾数位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "\n",
    "    tensor_copy = np.frombuffer(tensor_int8_writable.tobytes(), dtype=tensor_copy.dtype).reshape(tensor_copy.shape)\n",
    "    tensor_copy_writable = np.copy(tensor_copy)  # 创建一个可写的副本\n",
    "    return torch.from_numpy(tensor_copy_writable).to(device), error_indices  # Move tensor back to the original device\n",
    "\n",
    "def save_error_injected_model(model, err_rate, err_01, err_02, err_03):\n",
    "    err_checkpoint_dir = f\"/root/autodl-tmp/ERR_checkpoint/FP32/ERR_Rate_{err_rate}\"\n",
    "    os.makedirs(err_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    model_state_dict = model.state_dict()\n",
    "    error_dict = {}\n",
    "    for key, tensor in model_state_dict.items():\n",
    "        model_state_dict[key], error_indices = inject_error(tensor, err_rate, err_01, err_02, err_03)\n",
    "        error_dict[key] = error_indices\n",
    "\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    torch.save(model.state_dict(), f\"{err_checkpoint_dir}/model_ERR_01_{err_01}_ERR_02_{err_02}_ERR_03_{err_03}.pth\")\n",
    "\n",
    "    with open(f\"{err_checkpoint_dir}/error_log_ERR_01_{err_01}_ERR_02_{err_02}_ERR_03_{err_03}.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Parameter', 'Error Indices'])\n",
    "        for key, indices in error_dict.items():\n",
    "            writer.writerow([key, indices])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for err_rate in ERR_Rates:\n",
    "        print(f\"Processing error rate: {err_rate}\")\n",
    "        save_error_injected_model(model, err_rate, True, False, False)  # ERR_01 = True\n",
    "        save_error_injected_model(model, err_rate, False, True, False)  # ERR_02 = True\n",
    "        save_error_injected_model(model, err_rate, False, False, True)  # ERR_03 = True\n",
    "    \n",
    "    print(\"All experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF32\n",
    "\n",
    "同FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "precision = 'bf32'\n",
    "epochs = 1\n",
    "checkpoint_dir = f\"/root/autodl-tmp/checkpoint/{precision}\"\n",
    "model_path = f\"{checkpoint_dir}/model_epoch_{epochs-1}.pth\"\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=10)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device).bfloat16()\n",
    "\n",
    "ERR_Rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "def inject_error(tensor, err_rate, err_01, err_02, err_03):\n",
    "    tensor_copy = tensor.clone().float().cpu().numpy()  # Move to CPU and convert to float before numpy conversion\n",
    "    total_bits = tensor_copy.nbytes * 8\n",
    "    num_errors = int(total_bits * err_rate)\n",
    "\n",
    "    error_indices = []\n",
    "    tensor_bytes = tensor_copy.tobytes()\n",
    "    tensor_int8 = np.frombuffer(tensor_bytes, dtype=np.int8)\n",
    "    tensor_int8_writable = np.copy(tensor_int8)\n",
    "\n",
    "    for _ in range(num_errors):\n",
    "        bit_pos = random.randint(0, total_bits - 1)\n",
    "        byte_pos, bit_offset = divmod(bit_pos, 8)\n",
    "\n",
    "        if err_01 and bit_offset == 31:  # 符号位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "        elif err_02 and 23 <= bit_offset <= 30:  # 指数位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "        elif err_03 and bit_offset < 23:  # 尾数位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "\n",
    "    tensor_copy = np.frombuffer(tensor_int8_writable.tobytes(), dtype=tensor_copy.dtype).reshape(tensor_copy.shape)\n",
    "    tensor_copy_writable = np.copy(tensor_copy)\n",
    "    return torch.from_numpy(tensor_copy_writable).to(device).bfloat16(), error_indices  # Move back to GPU and convert to bfloat16\n",
    "\n",
    "def save_error_injected_model(model, err_rate, err_01, err_02, err_03):\n",
    "    err_checkpoint_dir = f\"/root/autodl-tmp/ERR_checkpoint/BF32/ERR_Rate_{err_rate}\"\n",
    "    os.makedirs(err_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    model_state_dict = model.state_dict()\n",
    "    error_dict = {}\n",
    "    for key, tensor in model_state_dict.items():\n",
    "        model_state_dict[key], error_indices = inject_error(tensor, err_rate, err_01, err_02, err_03)\n",
    "        error_dict[key] = error_indices\n",
    "\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    torch.save(model.state_dict(), f\"{err_checkpoint_dir}/model_ERR_01_{err_01}_ERR_02_{err_02}_ERR_03_{err_03}.pth\")\n",
    "\n",
    "    with open(f\"{err_checkpoint_dir}/error_log_ERR_01_{err_01}_ERR_02_{err_02}_ERR_03_{err_03}.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Parameter', 'Error Indices'])\n",
    "        for key, indices in error_dict.items():\n",
    "            writer.writerow([key, indices])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for err_rate in ERR_Rates:\n",
    "        print(f\"Processing error rate: {err_rate}\")\n",
    "        save_error_injected_model(model, err_rate, True, False, False)  # ERR_01 = True\n",
    "        save_error_injected_model(model, err_rate, False, True, False)  # ERR_02 = True\n",
    "        save_error_injected_model(model, err_rate, False, False, True)  # ERR_03 = True\n",
    "    \n",
    "    print(\"All experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP16\n",
    "\n",
    "类似FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "precision = 'fp16'\n",
    "epochs = 1\n",
    "checkpoint_dir = f\"/root/autodl-tmp/checkpoint/{precision}\"\n",
    "model_path = f\"{checkpoint_dir}/model_epoch_{epochs-1}.pth\"\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=10)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device).half()\n",
    "\n",
    "ERR_Rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "def inject_error(tensor, err_rate, err_01, err_02, err_03):\n",
    "    tensor = tensor.cpu()  # Move tensor to CPU for numpy operations\n",
    "    if tensor.dim() == 0:\n",
    "        # 处理标量值\n",
    "        tensor_copy = np.array([tensor.item()], dtype=np.float16)\n",
    "    else:\n",
    "        tensor_copy = tensor.clone().numpy().view(np.uint16)\n",
    "\n",
    "    total_bits = tensor_copy.nbytes * 8\n",
    "    num_errors = int(total_bits * err_rate)\n",
    "\n",
    "    error_indices = []\n",
    "    tensor_bytes = tensor_copy.tobytes()\n",
    "    tensor_uint16 = np.frombuffer(tensor_bytes, dtype=np.uint16)\n",
    "    tensor_uint16_writable = np.copy(tensor_uint16)  # 创建一个可写的副本\n",
    "\n",
    "    for _ in range(num_errors):\n",
    "        bit_pos = random.randint(0, total_bits - 1)\n",
    "        uint16_pos, bit_offset = divmod(bit_pos, 16)\n",
    "\n",
    "        if err_01 and bit_offset == 15:  # 符号位错误\n",
    "            tensor_uint16_writable[uint16_pos] ^= 1 << (bit_offset % 16)\n",
    "            error_indices.append(bit_pos)\n",
    "        elif err_02 and 10 <= bit_offset <= 14:  # 指数位错误\n",
    "            tensor_uint16_writable[uint16_pos] ^= 1 << (bit_offset % 16)\n",
    "            error_indices.append(bit_pos)\n",
    "        elif err_03 and bit_offset < 10:  # 尾数位错误\n",
    "            tensor_uint16_writable[uint16_pos] ^= 1 << (bit_offset % 16)\n",
    "            error_indices.append(bit_pos)\n",
    "\n",
    "    tensor_copy = np.frombuffer(tensor_uint16_writable.tobytes(), dtype=np.float16).reshape(tensor_copy.shape)\n",
    "    tensor_copy_writable = np.copy(tensor_copy)  # 创建一个可写的副本\n",
    "\n",
    "    if tensor.dim() == 0:\n",
    "        # 将标量值转换回张量\n",
    "        return torch.tensor(tensor_copy_writable[0], device=device), error_indices\n",
    "    else:\n",
    "        return torch.from_numpy(tensor_copy_writable).to(device), error_indices\n",
    "\n",
    "def save_error_injected_model(model, err_rate, err_01, err_02, err_03):\n",
    "    err_checkpoint_dir = f\"/root/autodl-tmp/ERR_checkpoint/FP16/ERR_Rate_{err_rate}\"\n",
    "    os.makedirs(err_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    model_state_dict = model.state_dict()\n",
    "    error_dict = {}\n",
    "    for key, tensor in model_state_dict.items():\n",
    "        model_state_dict[key], error_indices = inject_error(tensor, err_rate, err_01, err_02, err_03)\n",
    "        error_dict[key] = error_indices\n",
    "\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    torch.save(model.state_dict(), f\"{err_checkpoint_dir}/model_ERR_01_{err_01}_ERR_02_{err_02}_ERR_03_{err_03}.pth\")\n",
    "\n",
    "    with open(f\"{err_checkpoint_dir}/error_log_ERR_01_{err_01}_ERR_02_{err_02}_ERR_03_{err_03}.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Parameter', 'Error Indices'])\n",
    "        for key, indices in error_dict.items():\n",
    "            writer.writerow([key, indices])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for err_rate in ERR_Rates:\n",
    "        print(f\"Processing error rate: {err_rate}\")\n",
    "        save_error_injected_model(model, err_rate, True, False, False)  # ERR_01 = True\n",
    "        save_error_injected_model(model, err_rate, False, True, False)  # ERR_02 = True\n",
    "        save_error_injected_model(model, err_rate, False, False, True)  # ERR_03 = True\n",
    "    \n",
    "    print(\"All experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BF16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "precision = 'bf16'\n",
    "epochs = 1\n",
    "checkpoint_dir = f\"/root/autodl-tmp/checkpoint/{precision}\"\n",
    "model_path = f\"{checkpoint_dir}/model_epoch_{epochs-1}.pth\"\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=10)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device).bfloat16()\n",
    "\n",
    "ERR_Rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "def inject_error(tensor, err_rate, err_01, err_02):\n",
    "    tensor_copy = tensor.clone().float().cpu().numpy()  # Move to CPU and convert to float before numpy conversion\n",
    "    total_bits = tensor_copy.nbytes * 8\n",
    "    num_errors = int(total_bits * err_rate)\n",
    "\n",
    "    error_indices = []\n",
    "    tensor_bytes = tensor_copy.tobytes()\n",
    "    tensor_int8 = np.frombuffer(tensor_bytes, dtype=np.int8)\n",
    "    tensor_int8_writable = np.copy(tensor_int8)  # 创建一个可写的副本\n",
    "\n",
    "    for _ in range(num_errors):\n",
    "        bit_pos = random.randint(0, total_bits - 1)\n",
    "        byte_pos, bit_offset = divmod(bit_pos, 8)\n",
    "\n",
    "        if err_01 and bit_offset == 15:  # 符号位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "        elif err_02 and bit_offset < 15:  # 尾数位错误\n",
    "            tensor_int8_writable[byte_pos] ^= 1 << (bit_offset % 8)\n",
    "            error_indices.append(bit_pos)\n",
    "\n",
    "    tensor_copy = np.frombuffer(tensor_int8_writable.tobytes(), dtype=tensor_copy.dtype).reshape(tensor_copy.shape)\n",
    "    tensor_copy_writable = np.copy(tensor_copy)  # 创建一个可写的副本\n",
    "    return torch.from_numpy(tensor_copy_writable).to(device).bfloat16(), error_indices  # Move back to GPU and convert to bfloat16\n",
    "\n",
    "def save_error_injected_model(model, err_rate, err_01, err_02):\n",
    "    err_checkpoint_dir = f\"/root/autodl-tmp/ERR_checkpoint/BF16/ERR_Rate_{err_rate}\"\n",
    "    os.makedirs(err_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    model_state_dict = model.state_dict()\n",
    "    error_dict = {}\n",
    "    for key, tensor in model_state_dict.items():\n",
    "        model_state_dict[key], error_indices = inject_error(tensor, err_rate, err_01, err_02)\n",
    "        error_dict[key] = error_indices\n",
    "\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    torch.save(model.state_dict(), f\"{err_checkpoint_dir}/model_ERR_01_{err_01}_ERR_02_{err_02}.pth\")\n",
    "\n",
    "    with open(f\"{err_checkpoint_dir}/error_log_ERR_01_{err_01}_ERR_02_{err_02}.csv\", 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Parameter', 'Error Indices'])\n",
    "        for key, indices in error_dict.items():\n",
    "            writer.writerow([key, indices])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for err_rate in ERR_Rates:\n",
    "        print(f\"Processing error rate: {err_rate}\")\n",
    "        save_error_injected_model(model, err_rate, True, False)  # ERR_01 = True\n",
    "        save_error_injected_model(model, err_rate, False, True)  # ERR_02 = True\n",
    "    \n",
    "    print(\"All experiments completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Int8\n",
    "\n",
    "第一个cell是hf的量化包，理论上这是最高效的int8量化，但是没法直接用python操作底层存储，只能当成fp32进行错误注入，和实际向int8进行错误注入存在偏差\n",
    "\n",
    "第二个cell是模拟float32转成int8之后再进行错误注入的代码，实际底层还是float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义量化方式\n",
    "adc2 = []\n",
    "func2 = lambda x: (x / 128 - 1) * 0.6\n",
    "for i in range(0, 256):\n",
    "    adc2.append((func2(i), func2(i+1), func2(i+0.5), i))\n",
    "adc2[0] = (-999, adc2[0][1], adc2[0][2], adc2[0][3])\n",
    "adc2[-1] = (adc2[-1][0], 999, adc2[-1][2], adc2[-1][3])\n",
    "\n",
    "def encode1(origin):\n",
    "    return ((origin & 0xf) << 4) | (origin >> 4)\n",
    "\n",
    "def encode2(c):\n",
    "    c = (c & 0x55) << 1 | (c & 0xAA) >> 1\n",
    "    c = (c & 0x33) << 2 | (c & 0xCC) >> 2\n",
    "    c = (c & 0x0F) << 4 | (c & 0xF0) >> 4\n",
    "    return c\n",
    "\n",
    "def encode3(c):\n",
    "    return encode1(encode2(c))\n",
    "\n",
    "def encode(origin, err_mask, stuck_at_state):\n",
    "    err_mask00, stuck_at_state00 = err_mask, stuck_at_state\n",
    "    err_mask10, stuck_at_state10 = encode1(err_mask), encode1(stuck_at_state)\n",
    "    err_mask01, stuck_at_state01 = encode2(err_mask), encode2(stuck_at_state)\n",
    "    err_mask11, stuck_at_state11 = encode3(err_mask), encode3(stuck_at_state)\n",
    "\n",
    "    cor00 = (~err_mask00 & origin) | stuck_at_state00\n",
    "    cor01 = (~err_mask01 & origin) | stuck_at_state01\n",
    "    cor10 = (~err_mask10 & origin) | stuck_at_state10\n",
    "    cor11 = (~err_mask11 & origin) | stuck_at_state11\n",
    "\n",
    "    result = torch.stack((cor00, cor01, cor10, cor11))\n",
    "    diff_index = torch.argmin(torch.sum(torch.abs(result - origin), dim=1))\n",
    "    return result[diff_index]\n",
    "\n",
    "# 注入错误\n",
    "def inject_errors(params, width, err_rate):\n",
    "    for param in params:\n",
    "        if param.numel() < 1000:\n",
    "            continue\n",
    "\n",
    "        device = param.device  # 获取当前参数所在的设备\n",
    "\n",
    "        t = (0 * param.data).int().to(device)\n",
    "        for s, e, _, dig in adc2:\n",
    "            t += ((param.data >= s) * (param.data < e)).int() * dig\n",
    "\n",
    "        mask = torch.zeros(t.shape, device=device, dtype=torch.int32)\n",
    "        for jj in range(7):\n",
    "            bitmask = (torch.rand(mask.shape, device=device) < err_rate).int()\n",
    "            mask += bitmask * (1 << jj)\n",
    "\n",
    "        stuckat = ((torch.rand(t.shape, device=device) * 256).int()) & mask\n",
    "\n",
    "        t = t.view(-1)\n",
    "        mask = mask.view(-1)\n",
    "        stuckat = stuckat.view(-1)\n",
    "        for i0 in range(0, t.numel(), width):\n",
    "            t[i0:i0+width] = encode(t[i0:i0+width], mask[i0:i0+width], stuckat[i0:i0+width])\n",
    "\n",
    "        t = t.reshape(param.shape)\n",
    "        t2 = torch.zeros_like(param.data)\n",
    "        for _, _, ana, dig in adc2:\n",
    "            t2 += (t == dig).float() * ana\n",
    "\n",
    "        param.data = t2.float()\n",
    "\n",
    "# 主程序\n",
    "precision = 'fp32'\n",
    "epochs = 1\n",
    "checkpoint_dir = f\"/root/autodl-tmp/checkpoint/{precision}\"\n",
    "output_dir = \"/root/autodl-tmp/ERR_checkpoint/int8\"\n",
    "\n",
    "# 错误率列表\n",
    "ERR_Rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model_path = f\"{checkpoint_dir}/model_epoch_{epoch-1}.pth\"\n",
    "    \n",
    "    # 加载模型\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=10)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.float()\n",
    "    \n",
    "    # 对每个错误率进行处理\n",
    "    for err_rate in ERR_Rates:\n",
    "        # 创建模型的深拷贝，以便每个错误率都从原始模型开始\n",
    "        model_copy = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=10)\n",
    "        model_copy.load_state_dict(model.state_dict())\n",
    "        model_copy.float().to(device)\n",
    "        \n",
    "        # 获取模型参数并注入错误\n",
    "        params = [param for param in model_copy.parameters()]\n",
    "        inject_errors(params, width=1024, err_rate=err_rate)\n",
    "        \n",
    "        # 保存处理后的模型\n",
    "        output_path = f\"{output_dir}/model_epoch_{epoch-1}_err_rate_{err_rate:.1e}_int8.pth\"\n",
    "        torch.save(model_copy.state_dict(), output_path)\n",
    "        \n",
    "        print(f\"Processed and saved model for epoch {epoch}, error rate {err_rate:.1e}\")\n",
    "\n",
    "print(\"All models have been processed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 传统汉明码方法ECC检错修正\n",
    "\n",
    "使用了Hamming(72,64)码校验。\n",
    "\n",
    "1. 编码 (hamming_encode 函数):\n",
    "   - 输入64位数据，输出72位编码。\n",
    "   - 在72位编码中，位置 1, 2, 4, 8, 16, 32, 64 (即 2^n - 1) 被保留为校验位。\n",
    "   - 其他位置填充原始数据。\n",
    "   - 每个校验位通过异或运算计算，检查特定的数据位。\n",
    "   - 校验位的计算利用了位操作，每个校验位负责检查二进制表示中对应位为1的所有位置。\n",
    "\n",
    "2. 解码和纠错 (hamming_decode 函数):\n",
    "   - 输入72位编码数据。\n",
    "   - 计算syndrome（错误指示器）：\n",
    "     - 对每个校验位，重新计算其值并与接收到的值比较。\n",
    "     - 如果不匹配，将对应的位在syndrome中置1。\n",
    "   - 如果syndrome不为0：\n",
    "     - 如果syndrome值小于等于72，表示单比特错误，直接纠正对应位置。\n",
    "     - 如果syndrome值大于72，表示检测到双比特错误，无法纠正。\n",
    "   - 最后，提取原始64位数据。\n",
    "\n",
    "3. 错误注入和纠正过程 (在radom_err_resnet 函数中):\n",
    "   - 将模型参数转换为位字符串。\n",
    "   - 对每64位进行汉明编码。\n",
    "   - 随机注入错误，翻转一些比特。\n",
    "   - 使用汉明解码尝试恢复原始数据。\n",
    "   - 将恢复的数据重新加载到模型中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        # 加载预Train 的 ResNet-18 模型\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # 替换最后的全连接层，以匹配目标分类数量\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 初始化设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# Hamming(72,64) ECC编码\n",
    "def hamming_encode(data):\n",
    "    data = data.zfill(64)  # 如果数据长度不足64,在左侧填充0\n",
    "    encoded = ['0'] * 72\n",
    "    data_index = 0\n",
    "    for i in range(72):\n",
    "        if i == 0 or i == 1 or i == 3 or i == 7 or i == 15 or i == 31 or i == 63:\n",
    "            continue\n",
    "        if data_index < len(data):\n",
    "            encoded[i] = data[data_index]\n",
    "            data_index += 1\n",
    "    \n",
    "    for i in range(7):\n",
    "        parity = 0\n",
    "        for j in range(72):\n",
    "            if j & (1 << i):\n",
    "                parity ^= int(encoded[j])\n",
    "        encoded[2**i - 1] = str(parity)\n",
    "    \n",
    "    return ''.join(encoded)\n",
    "\n",
    "# Hamming(72,64) ECC解码和纠错\n",
    "def hamming_decode(encoded_data):\n",
    "    assert len(encoded_data) == 72\n",
    "    syndrome = 0\n",
    "    for i in range(7):\n",
    "        parity = 0\n",
    "        for j in range(72):\n",
    "            if j & (1 << i):\n",
    "                parity ^= int(encoded_data[j])\n",
    "        if parity:\n",
    "            syndrome |= (1 << i)\n",
    "    \n",
    "    if syndrome:\n",
    "        if syndrome <= 72:\n",
    "            # 单比特错误，纠正\n",
    "            corrected = list(encoded_data)\n",
    "            corrected[syndrome - 1] = str(1 - int(corrected[syndrome - 1]))\n",
    "            encoded_data = ''.join(corrected)\n",
    "            print(f\"检测到单比特错误，已纠正。错误位置: {syndrome - 1}\")\n",
    "        else:\n",
    "            print(\"检测到双比特错误，无法纠正。\")\n",
    "    \n",
    "    decoded = ''\n",
    "    for i in range(72):\n",
    "        if i != 0 and i != 1 and i != 3 and i != 7 and i != 15 and i != 31 and i != 63:\n",
    "            decoded += encoded_data[i]\n",
    "    \n",
    "    return decoded\n",
    "\n",
    "def radom_err_resnet(model, dataset_name, r=1e-4):\n",
    "    print(f\"\\n开始处理数据集: {dataset_name}, 错误率: {r:.1e}\")\n",
    "    \n",
    "    if dataset_name in [\"mnist\"]:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1))\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    data_path = \"./data\"\n",
    "    if dataset_name == \"mnist\":\n",
    "        test_dataset = MNIST(root=data_path, train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    print(f\"数据加载完成，Test 集大小: {len(test_dataset)}\")\n",
    "\n",
    "    print(\"开始处理模型参数...\")\n",
    "    with torch.no_grad():\n",
    "        all_params = torch.cat([param.view(-1) for param in model.parameters()])\n",
    "        param_bits = ''.join(format(b, '08b') for b in all_params.cpu().numpy().tobytes())\n",
    "        \n",
    "        encoded_params = [hamming_encode(param_bits[i:i+64]) for i in range(0, len(param_bits), 64)]\n",
    "        print(f\"ECC编码完成，编码后参数数量: {len(encoded_params)}\")\n",
    "        \n",
    "        error_count = 0\n",
    "        for i in range(len(encoded_params)):\n",
    "            for j in range(len(encoded_params[i])):\n",
    "                if random.random() < r:\n",
    "                    encoded_params[i] = encoded_params[i][:j] + str(1 - int(encoded_params[i][j])) + encoded_params[i][j+1:]\n",
    "                    error_count += 1\n",
    "        print(f\"模拟错误完成，总计引入错误: {error_count}\")\n",
    "        \n",
    "        decoded_params = ''.join(hamming_decode(p) for p in encoded_params)\n",
    "        print(\"ECC解码和纠错完成\")\n",
    "        \n",
    "        param_np = np.frombuffer(bytes(int(decoded_params[i:i+8], 2) for i in range(0, len(decoded_params), 8)), dtype=np.float32)\n",
    "        param_tensor = torch.from_numpy(param_np).to(device)\n",
    "        \n",
    "        # 将解码后的参数重新加载到模型中\n",
    "        start = 0\n",
    "        for param in model.parameters():\n",
    "            num_param = param.numel()\n",
    "            param.data = param_tensor[start:start+num_param].view(param.size())\n",
    "            start += num_param\n",
    "        print(f\"参数重新加载到模型完成\")\n",
    "\n",
    "    print(\"开始模型评估...\")\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"已评估 {batch_idx * len(data)} / {len(test_loader.dataset)} 个样本\")\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f\"评估完成。数据集: {dataset_name}, 错误率: {r:.1e}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "def process_error_rate(model, dataset_name, err_r):\n",
    "    radom_err_resnet(model, dataset_name, err_r)\n",
    "\n",
    "def process_dataset(dataset_name):\n",
    "    checkpoint_dir = \"/media/tangshi/AI0011/笔记/信工所/ECC校验/checkpoint/resnet18\"\n",
    "    log_dir = \"/media/tangshi/AI0011/笔记/信工所/ECC校验/log/resnet18\"\n",
    "\n",
    "    best_epoch = None\n",
    "    best_loss = float('inf')\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    log_file = os.path.join(log_dir, dataset_name, \"test_log.txt\")\n",
    "    with open(log_file, \"r\") as f:\n",
    "        content = f.read().strip()\n",
    "        if content:\n",
    "            loss, accuracy = content.split(',')\n",
    "            loss = float(loss.split(':')[1].strip())\n",
    "            accuracy = float(accuracy.split(':')[1].strip())\n",
    "            \n",
    "            print(f\"Dataset: {dataset_name}\")\n",
    "            print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            best_loss = loss\n",
    "            best_accuracy = accuracy\n",
    "            best_epoch = 10  # 假设Best epoch为10,你可以根据需要修改\n",
    "        else:\n",
    "            print(f\"Empty log file for {dataset_name} dataset.\")\n",
    "        \n",
    "        print(\"---\")\n",
    "\n",
    "    if best_epoch is not None:\n",
    "        checkpoint_file = os.path.join(checkpoint_dir, dataset_name, f\"checkpoint_epoch_{best_epoch}.pth\")\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            model = ResNet18().to(device)\n",
    "            model.load_state_dict(torch.load(checkpoint_file))\n",
    "            model.eval()\n",
    "            \n",
    "            error_rates = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "            \n",
    "            # 创建进程池\n",
    "            pool = mp.Pool(processes=mp.cpu_count())\n",
    "            \n",
    "            # 并行处理每个错误率\n",
    "            pool.starmap(process_error_rate, [(model, dataset_name, err_r) for err_r in error_rates])\n",
    "            \n",
    "            # 关闭进程池\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "            print(f\"Best Model for {dataset_name} Dataset (Epoch {best_epoch}):\")\n",
    "            print(f\"Test Loss: {best_loss:.4f}, Test Accuracy: {best_accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"Checkpoint file not found for {dataset_name} dataset.\")\n",
    "    else:\n",
    "        print(f\"No valid model found for {dataset_name} dataset.\")\n",
    "    \n",
    "    print(\"===\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = [\"mnist\"]\n",
    "    \n",
    "    # 创建进程池\n",
    "    pool = mp.Pool(processes=mp.cpu_count())\n",
    "    \n",
    "    # 并行处理每个数据集\n",
    "    pool.map(process_dataset, datasets)\n",
    "    \n",
    "    # 关闭进程池\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新ECC验证机制\n",
    "\n",
    "\n",
    "1. 奇偶校验位的计算：\n",
    "   代码中的 `ecc_encode` 函数使用了8个校验位。每个校验位是通过对数据位的特定子集进行异或（XOR）运算得到的。这实际上是一种并行的奇偶校验。\n",
    "\n",
    "2. 校验位的使用：\n",
    "   每个校验位负责检查数据位中的特定位置。这种方法允许不仅检测错误，还能定位错误。\n",
    "\n",
    "3. 错误检测：\n",
    "   在 `ecc_decode` 函数中，通过比较存储的校验位和重新计算的校验位来生成syndrome。如果syndrome不为零，就表示检测到了错误。\n",
    "\n",
    "4. 错误纠正：\n",
    "   如果检测到错误，代码会尝试通过syndrome的值来定位错误位置。这允许纠正单比特错误。\n",
    "\n",
    "5. 多位错误处理：\n",
    "   虽然这个方法主要用于纠正单比特错误，但它也能检测（但不能纠正）双比特错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        # 加载预Train 的 ResNet-18 模型\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # 替换最后的全连接层，以匹配目标分类数量\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# 初始化设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")\n",
    "# ECC编码函数\n",
    "def ecc_encode(data):\n",
    "    assert len(data) == 64  # 确保输入数据长度为64位\n",
    "    parity = [0] * 8  # 初始化8位校验位\n",
    "    for i in range(64):\n",
    "        for j in range(8):\n",
    "            if (i & (1 << j)) != 0:  # 检查第i位是否应该参与第j个校验位的计算\n",
    "                parity[j] ^= int(data[i])  # 异或运算计算校验位\n",
    "    return data + ''.join(map(str, parity))  # 返回数据和校验位的组合\n",
    "\n",
    "# ECC解码和纠错函数\n",
    "def ecc_decode(encoded_data):\n",
    "    data = encoded_data[:64]  # 提取原始数据\n",
    "    stored_parity = encoded_data[64:]  # 提取存储的校验位\n",
    "    \n",
    "    recalculated_parity = ecc_encode(data)[64:]  # 重新计算校验位\n",
    "    \n",
    "    # 计算syndrome（错误指示器）\n",
    "    syndrome = ''.join([str(int(a) ^ int(b)) for a, b in zip(stored_parity, recalculated_parity)])\n",
    "    \n",
    "    if syndrome == '00000000':\n",
    "        return data  # 无错误，直接返回数据\n",
    "    \n",
    "    error_position = int(syndrome, 2) - 1  # 将syndrome转换为错误位置\n",
    "    if 0 <= error_position < 64:\n",
    "        corrected_data = list(data)\n",
    "        corrected_data[error_position] = str(1 - int(data[error_position]))  # 翻转错误位\n",
    "        print(f\"检测到单比特错误，已纠正。错误位置: {error_position}\")\n",
    "        return ''.join(corrected_data)\n",
    "    \n",
    "    print(\"检测到双比特错误，无法纠正。\")\n",
    "    return data  # 无法纠正，返回原始数据\n",
    "\n",
    "def random_err_resnet_parallel(model, dataset_name, r=1e-4):\n",
    "    print(f\"\\n开始处理数据集: {dataset_name}, 错误率: {r:.1e}\")\n",
    "    \n",
    "    # 根据数据集选择适当的数据变换\n",
    "    if dataset_name in [\"mnist\"]:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # 将单通道图像转换为3通道\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "    # 加载数据集\n",
    "    data_path = \"./data\"\n",
    "    if dataset_name == \"mnist\":\n",
    "        test_dataset = MNIST(root=data_path, train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "    print(f\"数据加载完成，Test 集大小: {len(test_dataset)}\")\n",
    "\n",
    "    def process_param(name, param):\n",
    "        print(f\"处理参数: {name}, 形状: {param.shape}\")\n",
    "        # 将参数转换为位字符串\n",
    "        param_bits = ''.join(format(b, '08b') for b in param.cpu().numpy().tobytes())\n",
    "        \n",
    "        # 对参数进行ECC编码\n",
    "        encoded_params = [ecc_encode(param_bits[i:i+64]) for i in range(0, len(param_bits), 64)]\n",
    "        print(f\"ECC编码完成，编码后参数数量: {len(encoded_params)}\")\n",
    "        \n",
    "        # 模拟随机错误\n",
    "        error_count = 0\n",
    "        for i in range(len(encoded_params)):\n",
    "            for j in range(len(encoded_params[i])):\n",
    "                if random.random() < r:\n",
    "                    encoded_params[i] = encoded_params[i][:j] + str(1 - int(encoded_params[i][j])) + encoded_params[i][j+1:]\n",
    "                    error_count += 1\n",
    "        print(f\"模拟错误完成，总计引入错误: {error_count}\")\n",
    "        \n",
    "        # ECC解码和纠错\n",
    "        decoded_params = ''.join(ecc_decode(p) for p in encoded_params)\n",
    "        print(\"ECC解码和纠错完成\")\n",
    "        \n",
    "        # 将解码后的参数重新加载到模型中\n",
    "        param_np = np.frombuffer(bytes(int(decoded_params[i:i+8], 2) for i in range(0, len(decoded_params), 8)), dtype=np.float32)\n",
    "        param.data = torch.from_numpy(param_np).view(param.size()).to(device)\n",
    "        print(f\"参数重新加载到模型完成\")\n",
    "\n",
    "    print(\"开始处理模型参数...\")\n",
    "    with torch.no_grad():\n",
    "        # 创建进程池\n",
    "        pool = mp.Pool(processes=mp.cpu_count())\n",
    "        \n",
    "        # 并行处理每个参数\n",
    "        pool.starmap(process_param, model.named_parameters())\n",
    "        \n",
    "        # 关闭进程池\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    print(\"开始模型评估...\")\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"已评估 {batch_idx * len(data)} / {len(test_loader.dataset)} 个样本\")\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f\"评估完成。数据集: {dataset_name}, 错误率: {r:.1e}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "def process_error_rate(model, dataset_name, err_r):\n",
    "    random_err_resnet_parallel(model, dataset_name, err_r)\n",
    "\n",
    "def process_dataset(dataset_name):\n",
    "    checkpoint_dir = \"/media/tangshi/AI0011/笔记/信工所/ECC校验/checkpoint/resnet18\"\n",
    "    log_dir = \"/media/tangshi/AI0011/笔记/信工所/ECC校验/log/resnet18\"\n",
    "\n",
    "    best_epoch = None\n",
    "    best_loss = float('inf')\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    log_file = os.path.join(log_dir, dataset_name, \"test_log.txt\")\n",
    "    with open(log_file, \"r\") as f:\n",
    "        content = f.read().strip()\n",
    "        if content:\n",
    "            loss, accuracy = content.split(',')\n",
    "            loss = float(loss.split(':')[1].strip())\n",
    "            accuracy = float(accuracy.split(':')[1].strip())\n",
    "            \n",
    "            print(f\"Dataset: {dataset_name}\")\n",
    "            print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            best_loss = loss\n",
    "            best_accuracy = accuracy\n",
    "            best_epoch = 10  # 假设Best epoch为10,你可以根据需要修改\n",
    "        else:\n",
    "            print(f\"Empty log file for {dataset_name} dataset.\")\n",
    "        \n",
    "        print(\"---\")\n",
    "\n",
    "    if best_epoch is not None:\n",
    "        checkpoint_file = os.path.join(checkpoint_dir, dataset_name, f\"checkpoint_epoch_{best_epoch}.pth\")\n",
    "        if os.path.exists(checkpoint_file):\n",
    "            model = ResNet18().to(device)\n",
    "            model.load_state_dict(torch.load(checkpoint_file))\n",
    "            model.eval()\n",
    "            \n",
    "            error_rates = [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]\n",
    "            \n",
    "            # 创建进程池\n",
    "            pool = mp.Pool(processes=mp.cpu_count())\n",
    "            \n",
    "            # 并行处理每个错误率\n",
    "            pool.starmap(process_error_rate, [(model, dataset_name, err_r) for err_r in error_rates])\n",
    "            \n",
    "            # 关闭进程池\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            \n",
    "            print(f\"Best Model for {dataset_name} Dataset (Epoch {best_epoch}):\")\n",
    "            print(f\"Test Loss: {best_loss:.4f}, Test Accuracy: {best_accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"Checkpoint file not found for {dataset_name} dataset.\")\n",
    "    else:\n",
    "        print(f\"No valid model found for {dataset_name} dataset.\")\n",
    "    \n",
    "    print(\"===\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = [\"mnist\"]\n",
    "    \n",
    "    # 创建进程池\n",
    "    pool = mp.Pool(processes=mp.cpu_count())\n",
    "    \n",
    "    # 并行处理每个数据集\n",
    "    pool.map(process_dataset, datasets)\n",
    "    \n",
    "    # 关闭进程池\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECCECCECCEECECCCEEEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Train Batch: 0/938 Loss: 2.340935\n",
      "Train Batch: 100/938 Loss: 0.034722\n",
      "Train Batch: 200/938 Loss: 0.249568\n",
      "Train Batch: 300/938 Loss: 0.154008\n",
      "Train Batch: 400/938 Loss: 0.037314\n",
      "Train Batch: 500/938 Loss: 0.008300\n",
      "Train Batch: 600/938 Loss: 0.020768\n",
      "Train Batch: 700/938 Loss: 0.038694\n",
      "Train Batch: 800/938 Loss: 0.110554\n",
      "Train Batch: 900/938 Loss: 0.123319\n",
      "Test Accuracy: 98.78%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载MNIST数据集\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 定义模型\n",
    "model = resnet18(pretrained=False)\n",
    "model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Linear(512, 10)\n",
    "model = model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练函数\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Batch: {batch_idx}/{len(train_loader)} Loss: {loss.item():.6f}')\n",
    "\n",
    "# 测试函数\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# 训练模型\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    test(model, test_loader, device)\n",
    "\n",
    "# 保存原始模型\n",
    "torch.save(model.state_dict(), 'original_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Performance:\n",
      "Test Accuracy: 98.78%\n",
      "ERR_TYPE: 0, ERR_RATE: 0.1\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 9.82%\n",
      "Accuracy difference: 88.96%\n",
      "\n",
      "ERR_TYPE: 0, ERR_RATE: 0.01\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 21.26%\n",
      "Accuracy difference: 77.52%\n",
      "\n",
      "ERR_TYPE: 0, ERR_RATE: 0.001\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.87%\n",
      "Accuracy difference: 0.09%\n",
      "\n",
      "ERR_TYPE: 0, ERR_RATE: 0.0001\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.80%\n",
      "Accuracy difference: 0.02%\n",
      "\n",
      "ERR_TYPE: 0, ERR_RATE: 1e-05\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 0, ERR_RATE: 1e-06\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 0, ERR_RATE: 1e-07\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 0, ERR_RATE: 1e-08\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 0, ERR_RATE: 1e-09\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 1, ERR_RATE: 0.1\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 11.35%\n",
      "Accuracy difference: 87.43%\n",
      "\n",
      "ERR_TYPE: 1, ERR_RATE: 0.01\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 89.56%\n",
      "Accuracy difference: 9.22%\n",
      "\n",
      "ERR_TYPE: 1, ERR_RATE: 0.001\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.84%\n",
      "Accuracy difference: 0.06%\n",
      "\n",
      "ERR_TYPE: 1, ERR_RATE: 0.0001\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 1, ERR_RATE: 1e-05\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 1, ERR_RATE: 1e-06\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 1, ERR_RATE: 1e-07\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 1, ERR_RATE: 1e-08\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 1, ERR_RATE: 1e-09\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.79%\n",
      "Accuracy difference: 0.01%\n",
      "\n",
      "ERR_TYPE: 2, ERR_RATE: 0.1\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 31.41%\n",
      "Accuracy difference: 67.37%\n",
      "\n",
      "ERR_TYPE: 2, ERR_RATE: 0.01\n",
      "Quantized Model Performance:\n",
      "Test Accuracy: 98.89%\n",
      "Accuracy difference: 0.11%\n",
      "\n",
      "ERR_TYPE: 2, ERR_RATE: 0.001\n",
      "Quantized Model Performance:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 加载MNIST测试数据集\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def ERR(q_tensor, ERR_TYPE, ERR_RATE):\n",
    "    # 确保张量在 GPU 上\n",
    "    if not q_tensor.is_cuda:\n",
    "        q_tensor = q_tensor.cuda()\n",
    "\n",
    "    # 将张量转换为字节表示\n",
    "    np_arr = q_tensor.cpu().numpy()\n",
    "    byte_arr = np.frombuffer(np_arr.data, dtype=np.uint8)\n",
    "    byte_tensor = torch.from_numpy(byte_arr).cuda()\n",
    "\n",
    "    # 创建错误掩码\n",
    "    error_mask = torch.rand(byte_tensor.shape, device=byte_tensor.device) < ERR_RATE\n",
    "\n",
    "    if ERR_TYPE == \"0\":\n",
    "        # 在第一位（符号位）注入错误\n",
    "        flip_mask = error_mask & (byte_tensor & 0b10000000 != 0)\n",
    "        byte_tensor = byte_tensor ^ (flip_mask.to(torch.uint8) << 7)\n",
    "    elif ERR_TYPE == \"1\":\n",
    "        # 在 2、3、4 位注入错误\n",
    "        for i in range(5, 7):\n",
    "            flip_mask = error_mask & ((byte_tensor & (1 << i)) != 0)\n",
    "            byte_tensor = byte_tensor ^ (flip_mask.to(torch.uint8) << i)\n",
    "    elif ERR_TYPE == \"2\":\n",
    "        # 在 5、6、7、8 位注入错误\n",
    "        for i in range(4):\n",
    "            flip_mask = error_mask & ((byte_tensor & (1 << i)) != 0)\n",
    "            byte_tensor = byte_tensor ^ (flip_mask.to(torch.uint8) << i)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid ERR_TYPE. Must be '0', '1', or '2'.\")\n",
    "\n",
    "    # 将字节张量转回原始数据类型\n",
    "    byte_arr_with_error = byte_tensor.cpu().numpy()\n",
    "    np_arr_with_error = np.frombuffer(byte_arr_with_error.data, dtype=np_arr.dtype)\n",
    "    q_tensor_with_error = torch.from_numpy(np_arr_with_error).cuda().view(q_tensor.shape)\n",
    "\n",
    "    return q_tensor_with_error\n",
    "\n",
    "# 修改 quantize 函数以使用新的 ERR 函数\n",
    "def quantize(tensor, ERR_TYPE, ERR_RATE, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    scale = (tensor.max() - tensor.min()) / (qmax - qmin)\n",
    "    zero_point = qmin - tensor.min() / scale\n",
    "    q_tensor = torch.round(tensor / scale + zero_point)\n",
    "    q_tensor.clamp_(qmin, qmax)\n",
    "    q_tensor = ERR(q_tensor.to(torch.uint8), ERR_TYPE, ERR_RATE)  # 确保输入是 uint8\n",
    "    return q_tensor.to(tensor.dtype), scale, zero_point  # 转回原始数据类型\n",
    "\n",
    "# 反量化函数\n",
    "def dequantize(q_tensor, scale, zero_point):\n",
    "    return scale * (q_tensor - zero_point)\n",
    "\n",
    "# 测试函数\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# 加载原始模型\n",
    "original_model = resnet18(pretrained=False)\n",
    "original_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "original_model.fc = nn.Linear(512, 10)\n",
    "original_model.load_state_dict(torch.load('original_model.pth'))\n",
    "original_model = original_model.to(device)\n",
    "\n",
    "print(\"Original Model Performance:\")\n",
    "original_accuracy = test(original_model, test_loader, device)\n",
    "\n",
    "# 量化模型并评估\n",
    "for ERR_TYPE in [\"0\", \"1\", \"2\"]:\n",
    "    # for ERR_RATE in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9]:\n",
    "    for ERR_RATE in [1e-1, 1e-2, 1e-3, 1e-4]:\n",
    "        print(f\"ERR_TYPE: {ERR_TYPE}, ERR_RATE: {ERR_RATE}\")\n",
    "        \n",
    "        quantized_model = resnet18(pretrained=False)\n",
    "        quantized_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        quantized_model.fc = nn.Linear(512, 10)\n",
    "        quantized_model.load_state_dict(torch.load('original_model.pth'))\n",
    "        quantized_model = quantized_model.to(device)\n",
    "        \n",
    "        for name, param in quantized_model.named_parameters():\n",
    "            q_param, scale, zero_point = quantize(param.data, ERR_TYPE, ERR_RATE)\n",
    "            param.data = dequantize(q_param, scale, zero_point)\n",
    "\n",
    "        print(\"Quantized Model Performance:\")\n",
    "        quantized_accuracy = test(quantized_model, test_loader, device)\n",
    "\n",
    "        print(f\"Accuracy difference: {abs(original_accuracy - quantized_accuracy):.2f}%\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LsData",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
